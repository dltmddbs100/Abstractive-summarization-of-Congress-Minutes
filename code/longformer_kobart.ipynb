{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "longformer_kobart.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHiye06UUwv2"
      },
      "source": [
        "import os\n",
        "from tqdm import tqdm, trange\n",
        "import time\n",
        "import re\n",
        "import copy\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda import amp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from transformers import BartForConditionalGeneration,AutoTokenizer, BartConfig\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
        "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding\n",
        "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
        "\n",
        "from dataset import KoBARTSummaryDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DCEhkaXYNFT"
      },
      "source": [
        "data_path='/content/drive/MyDrive/Dacon/한국어 생성요약/data/235813_AI 기반 회의 녹취록 요약 경진대회_data/'\n",
        "model_path='/content/drive/MyDrive/Dacon/한국어 생성요약/LG_model/'\n",
        "sub_path='/content/drive/MyDrive/Dacon/한국어 생성요약/LG_sub/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBKf_o69YsV7"
      },
      "source": [
        "# LongformerKoBART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKFG0bkiVX0-"
      },
      "source": [
        "# replaces Kobart's attention layer\n",
        "\n",
        "class LongformerSelfAttentionForBart(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.longformer_self_attn = LongformerSelfAttention(config, layer_id=layer_id)\n",
        "        self.output = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "\n",
        "    # be able to receive the same type of input as the existing layer of kobart and make the same type of output\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        key_value_states: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "\n",
        "        # bs x seq_len x seq_len -> bs x seq_len 으로 변경\n",
        "        attention_mask = attention_mask.squeeze(dim=1)\n",
        "        attention_mask = attention_mask[:,0]\n",
        "\n",
        "        is_index_masked = attention_mask < 0\n",
        "        is_index_global_attn = attention_mask > 0\n",
        "        is_global_attn = is_index_global_attn.flatten().any().item()\n",
        "\n",
        "        outputs = self.longformer_self_attn(\n",
        "            hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=None,\n",
        "            is_index_masked=is_index_masked,\n",
        "            is_index_global_attn=is_index_global_attn,\n",
        "            is_global_attn=is_global_attn,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "\n",
        "        attn_output = self.output(outputs[0])\n",
        "\n",
        "        return (attn_output,) + outputs[1:] if len(outputs) == 2 else (attn_output, None, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI1qVj5b0GQK"
      },
      "source": [
        "# code used when calling from_pretrained after saving the model\n",
        "# redefine the embedding layer because it is not created according to the config\n",
        "\n",
        "class LongformerEncoderDecoderForConditionalGeneration(BartForConditionalGeneration):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        \n",
        "        if config.attention_mode == 'n2':\n",
        "            pass  # do nothing, use BertSelfAttention instead\n",
        "        else:\n",
        "\n",
        "            self.model.encoder.embed_positions = BartLearnedPositionalEmbedding(\n",
        "                config.max_encoder_position_embeddings, \n",
        "                config.d_model, \n",
        "                config.pad_token_id)\n",
        "\n",
        "            self.model.decoder.embed_positions = BartLearnedPositionalEmbedding(\n",
        "                config.max_decoder_position_embeddings, \n",
        "                config.d_model, \n",
        "                config.pad_token_id)\n",
        "\n",
        "            for i, layer in enumerate(self.model.encoder.layers):\n",
        "                layer.self_attn = LongformerSelfAttentionForBart(config, layer_id=i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61lkEZff0XES"
      },
      "source": [
        "# longformer bart model config creation class\n",
        "\n",
        "class LongformerEncoderDecoderConfig(BartConfig):\n",
        "    def __init__(self, attention_window: List[int] = None, attention_dilation: List[int] = None,\n",
        "                 autoregressive: bool = False, attention_mode: str = 'sliding_chunks',\n",
        "                 gradient_checkpointing: bool = False, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            attention_window: list of attention window sizes of length = number of layers.\n",
        "                window size = number of attention locations on each side.\n",
        "                For an affective window size of 512, use `attention_window=[256]*num_layers`\n",
        "                which is 256 on each side.\n",
        "            attention_dilation: list of attention dilation of length = number of layers.\n",
        "                attention dilation of `1` means no dilation.\n",
        "            autoregressive: do autoregressive attention or have attention of both sides\n",
        "            attention_mode: 'n2' for regular n^2 self-attention, 'tvm' for TVM implemenation of Longformer\n",
        "                selfattention, 'sliding_chunks' for another implementation of Longformer selfattention\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention_window = attention_window\n",
        "        self.attention_dilation = attention_dilation\n",
        "        self.autoregressive = autoregressive\n",
        "        self.attention_mode = attention_mode\n",
        "        self.gradient_checkpointing = gradient_checkpointing\n",
        "        assert self.attention_mode in ['tvm', 'sliding_chunks', 'n2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2wNPNR5YkFy"
      },
      "source": [
        "args={}\n",
        "args['train_path']=data_path+'train_evi_concat_final.csv'\n",
        "args['test_path']=data_path+'test_evi_final.csv'\n",
        "args['weight_path']=model_path\n",
        "args['batch_size']=4\n",
        "args['num_workers']=4\n",
        "args['max_epochs']=4\n",
        "args['attention_window'] = 512\n",
        "args['max_pos'] = 2052\n",
        "args['max_seq_len'] = 2048"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4v4Se0FvjCz"
      },
      "source": [
        "# load tokenizer, model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hyunwoongko/kobart\", model_max_length=args['max_pos'])\n",
        "model = BartForConditionalGeneration.from_pretrained(\"hyunwoongko/kobart\")\n",
        "config = LongformerEncoderDecoderConfig.from_pretrained(\"hyunwoongko/kobart\")\n",
        "\n",
        "model.config = config\n",
        "\n",
        "# in BART attention_probs_dropout_prob is attention_dropout, but LongformerSelfAttention\n",
        "# expects attention_probs_dropout_prob, so set it here\n",
        "config.attention_probs_dropout_prob = config.attention_dropout\n",
        "config.architectures = ['LongformerEncoderDecoderForConditionalGeneration']\n",
        "\n",
        "# extend position embeddings\n",
        "tokenizer.model_max_length = args['max_pos']\n",
        "tokenizer.init_kwargs['model_max_length'] = args['max_pos']\n",
        "current_max_pos, embed_size = model.model.encoder.embed_positions.weight.shape\n",
        "assert current_max_pos == config.max_position_embeddings + 2\n",
        "\n",
        "config.max_encoder_position_embeddings = args['max_pos']\n",
        "config.max_decoder_position_embeddings = config.max_position_embeddings\n",
        "del config.max_position_embeddings\n",
        "args['max_pos'] += 2  # NOTE: BART has positions 0,1 reserved, so embedding size is max position + 2\n",
        "assert args['max_pos'] >= current_max_pos\n",
        "\n",
        "# allocate a larger position embedding matrix for the encoder\n",
        "new_encoder_pos_embed = model.model.encoder.embed_positions.weight.new_empty(args['max_pos'], embed_size)\n",
        "# copy position embeddings over and over to initialize the new position embeddings\n",
        "k = 2\n",
        "step = current_max_pos - 2\n",
        "while k < args['max_pos'] - 1:\n",
        "    new_encoder_pos_embed[k:(k + step)] = model.model.encoder.embed_positions.weight[2:]\n",
        "    k += step\n",
        "model.model.encoder.embed_positions.weight.data = new_encoder_pos_embed\n",
        "\n",
        "# allocate a larger position embedding matrix for the decoder\n",
        "# new_decoder_pos_embed = model.model.decoder.embed_positions.weight.new_empty(args['max_pos'], embed_size)\n",
        "# # copy position embeddings over and over to initialize the new position embeddings\n",
        "# k = 2\n",
        "# step = current_max_pos - 2\n",
        "# while k < args['max_pos'] - 1:\n",
        "#     new_decoder_pos_embed[k:(k + step)] = model.model.decoder.embed_positions.weight[2:]\n",
        "#     k += step\n",
        "# model.model.decoder.embed_positions.weight.data = new_decoder_pos_embed\n",
        "\n",
        "# replace the `modeling_bart.SelfAttention` object with `LongformerSelfAttention`\n",
        "config.attention_window = [args['attention_window']] * config.num_hidden_layers\n",
        "config.attention_dilation = [1] * config.num_hidden_layers\n",
        "\n",
        "for i, layer in enumerate(model.model.encoder.layers):\n",
        "    longformer_self_attn_for_bart = LongformerSelfAttentionForBart(config, layer_id=i)\n",
        "\n",
        "    longformer_self_attn_for_bart.longformer_self_attn.query = layer.self_attn.q_proj\n",
        "    longformer_self_attn_for_bart.longformer_self_attn.key = layer.self_attn.k_proj\n",
        "    longformer_self_attn_for_bart.longformer_self_attn.value = layer.self_attn.v_proj\n",
        "\n",
        "    longformer_self_attn_for_bart.longformer_self_attn.query_global = copy.deepcopy(layer.self_attn.q_proj)\n",
        "    longformer_self_attn_for_bart.longformer_self_attn.key_global = copy.deepcopy(layer.self_attn.k_proj)\n",
        "    longformer_self_attn_for_bart.longformer_self_attn.value_global = copy.deepcopy(layer.self_attn.v_proj)\n",
        "\n",
        "    longformer_self_attn_for_bart.output = layer.self_attn.out_proj\n",
        "\n",
        "    layer.self_attn = longformer_self_attn_for_bart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgJJHbGwYk7M",
        "outputId": "c5a9e92e-94da-4bb6-dfc7-8a70803eb6ce"
      },
      "source": [
        "torch.manual_seed(1514)\n",
        "\n",
        "train_data = KoBARTSummaryDataset(args['train_path'],tok=AutoTokenizer.from_pretrained(\"hyunwoongko/kobart\"),max_len=args['max_seq_len'])\n",
        "train_data, eval_data = torch.utils.data.random_split(train_data,[2400,594])\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              batch_size=args['batch_size'], \n",
        "                              shuffle=True,\n",
        "                              num_workers=args['num_workers'], \n",
        "                              pin_memory=True)\n",
        "\n",
        "eval_dataloader = DataLoader(eval_data,\n",
        "                              batch_size=args['batch_size'], \n",
        "                              shuffle=False,\n",
        "                              num_workers=args['num_workers'], \n",
        "                              pin_memory=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jCqz-nkKfg1"
      },
      "source": [
        "def accuracy_function(real, pred):\n",
        "    accuracies = torch.eq(real, torch.argmax(pred, dim=2))\n",
        "    mask = torch.logical_not(torch.eq(real, -100))\n",
        "    accuracies = torch.logical_and(mask, accuracies)\n",
        "    accuracies = accuracies.clone().detach()\n",
        "    mask = mask.clone().detach()\n",
        "\n",
        "    return torch.sum(accuracies)/torch.sum(mask)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = torch.logical_not(torch.eq(real, -100))\n",
        "    loss_ = criterion(pred.permute(0,2,1), real)\n",
        "    mask = mask.clone().detach()\n",
        "    loss_ = mask * loss_\n",
        "\n",
        "    return torch.sum(loss_)/torch.sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jKSFTuuKfdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5428617f-448e-43ae-963b-21fffb6f0b37"
      },
      "source": [
        "model=model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(),lr=2e-5, weight_decay=1e-4,correct_bias=False)\n",
        "scaler = amp.GradScaler()\n",
        "\n",
        "\n",
        "for epoch_i in range(0, args['max_epochs']):\n",
        "  \n",
        "  # ========================================\n",
        "  #               Training\n",
        "  # ========================================\n",
        "\n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, args['max_epochs']))\n",
        "  model.train()\n",
        "  t0 = time.time()\n",
        "  total_train_loss = 0\n",
        "  total_train_acc = 0\n",
        "  total_batch=len(train_dataloader)\n",
        "\n",
        "  for i, batch in enumerate(train_dataloader):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
        "    decoder_attention_mask = batch['decoder_attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    model.model.encoder.config.gradient_checkpointing = True\n",
        "    model.model.decoder.config.gradient_checkpointing = True\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with amp.autocast():\n",
        "      output = model(input_ids=input_ids,\n",
        "                   attention_mask=attention_mask,\n",
        "                   decoder_input_ids=decoder_input_ids,\n",
        "                   decoder_attention_mask=decoder_attention_mask,\n",
        "                   labels=labels)\n",
        "      loss = output.loss\n",
        "\n",
        "    acc = accuracy_function(labels, output.logits)\n",
        "\n",
        "    total_train_acc += acc  \n",
        "    total_train_loss += loss\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "      \n",
        "    training_time = time.time() - t0\n",
        "\n",
        "    print(f\"\\rTotal Batch {i+1}/{total_batch} , elapsed time : {training_time/(i+1):.1f}s , train_loss : {total_train_loss/(i+1):.2f}, train_acc: {total_train_acc/(i+1):.3f}\", end='')\n",
        "  print(\"\")\n",
        "\n",
        "  model.eval()\n",
        "  total_eval_loss=0\n",
        "  total_val_acc=0\n",
        "  total_val_batch=len(eval_dataloader)\n",
        "\n",
        "  for i,batch in enumerate(eval_dataloader):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
        "    decoder_attention_mask = batch['decoder_attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids=input_ids,\n",
        "                   attention_mask=attention_mask,\n",
        "                   decoder_input_ids=decoder_input_ids,\n",
        "                   decoder_attention_mask=decoder_attention_mask,\n",
        "                   labels=labels)\n",
        "      loss = output.loss\n",
        "    \n",
        "    acc = accuracy_function(labels, output.logits)\n",
        "    \n",
        "    total_val_acc +=acc\n",
        "    total_eval_loss += loss\n",
        "    \n",
        "    print(f\"\\rValidation Batch {i+1}/{total_val_batch} , validation loss: {total_eval_loss/(i+1):.2f}, val_acc: {total_val_acc/(i+1):.3f}\", end='')\n",
        "\n",
        "  torch.save(model.state_dict(), args['weight_path']+'LongformerKoBART_2048_epoch {} weight.ckpt'.format(epoch_i + 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Total Batch 600/600 , elapsed time : 1.2s , train_loss : 0.98, train_acc: 0.810\n",
            "Validation Batch 149/149 , validation loss: 0.83, val_acc: 0.832\n",
            "======== Epoch 2 / 4 ========\n",
            "Total Batch 600/600 , elapsed time : 1.2s , train_loss : 0.56, train_acc: 0.873\n",
            "Validation Batch 149/149 , validation loss: 0.82, val_acc: 0.836\n",
            "======== Epoch 3 / 4 ========\n",
            "Total Batch 600/600 , elapsed time : 1.2s , train_loss : 0.40, train_acc: 0.900\n",
            "Validation Batch 149/149 , validation loss: 0.85, val_acc: 0.836\n",
            "======== Epoch 4 / 4 ========\n",
            "Total Batch 600/600 , elapsed time : 1.2s , train_loss : 0.30, train_acc: 0.922\n",
            "Validation Batch 149/149 , validation loss: 0.88, val_acc: 0.836"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation\n",
        "\n",
        "+ Generate(sets, max_length=200, no_repeat_ngram_size=3)"
      ],
      "metadata": {
        "id": "TxgVTv70x1p2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyP3ns80k-jG",
        "outputId": "39adcf16-4dc9-4717-dae1-6744a23b0ecb"
      },
      "source": [
        "model.load_state_dict(torch.load(args['weight_path']+'LongformerKoBART_2048_epoch 2 weight.ckpt'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(30000, 768, padding_idx=3)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartEncoderLayer(\n",
              "          (self_attn): LongformerSelfAttentionForBart(\n",
              "            (longformer_self_attn): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartEncoderLayer(\n",
              "          (self_attn): LongformerSelfAttentionForBart(\n",
              "            (longformer_self_attn): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartEncoderLayer(\n",
              "          (self_attn): LongformerSelfAttentionForBart(\n",
              "            (longformer_self_attn): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartEncoderLayer(\n",
              "          (self_attn): LongformerSelfAttentionForBart(\n",
              "            (longformer_self_attn): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartEncoderLayer(\n",
              "          (self_attn): LongformerSelfAttentionForBart(\n",
              "            (longformer_self_attn): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartEncoderLayer(\n",
              "          (self_attn): LongformerSelfAttentionForBart(\n",
              "            (longformer_self_attn): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test data load\n",
        "test_data = KoBARTSummaryDataset(args['test_path'],tok=AutoTokenizer.from_pretrained(\"hyunwoongko/kobart\"),max_len=args['max_len'],infer=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                              batch_size=args['batch_size'], \n",
        "                              shuffle=False,\n",
        "                              num_workers=args['num_workers'], \n",
        "                              pin_memory=True)"
      ],
      "metadata": {
        "id": "iR994ShYzALd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8pXzkMtAJtv",
        "outputId": "3b78490d-2769-4809-a56d-11617e7f1a00"
      },
      "source": [
        "# prediction\n",
        "test_sum=[]\n",
        "\n",
        "for i,batch in tqdm(enumerate(test_dataloader)):\n",
        "  sets=batch['input_ids'].to(device)\n",
        "  batch_sum=model.generate(sets, max_length=200,no_repeat_ngram_size=3)\n",
        "  test_sum=[*test_sum,*batch_sum]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127it [01:21,  1.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjteT5F6AJtv"
      },
      "source": [
        "test_sum_sent=[tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in test_sum]\n",
        "\n",
        "sub=pd.read_csv(data_path+'sample_submission.csv')\n",
        "sub['summary']=test_sum_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub.to_csv(sub_path+'LongformerKoBART_2048_epoch 2.csv',index=False)"
      ],
      "metadata": {
        "id": "EFVgZgTSzIKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View sub"
      ],
      "metadata": {
        "id": "00b6dcH8xxu0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhId3VW7vuYI"
      },
      "source": [
        "test_real=pd.read_csv(data_path+'test_evi_final.csv')\n",
        "sub=pd.read_csv(sub_path+'kobart_epoch2_ml_1024_evi_v02_g3.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "PSz2pEt1nzsY",
        "outputId": "51ead813-d4c0-4754-9c11-1273d006bcb5"
      },
      "source": [
        "id=27\n",
        "print('Context:\\n')\n",
        "display(test_real['title_context'][id])\n",
        "\n",
        "print('\\nsummary:\\n')\n",
        "display(sub['summary'][id])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'제296회 본회의 제1차: 의사일정 제9항, 음성군 참전유공자 지원 조례 일부개정조례안, 의사일정10항, 음성군 국가보훈대상자 예우 및 지원에 관한 조례 일부개정조례안을 일괄 상정합니다. 주민지원과장께서는 나오셔서 2건의 안건에 대하여 일괄 제안설명하여 주시기 바랍니다. 주민지원과장입니다. 음성군 참전유공자 지원 조례 일부개정조례안에 대하여 설명드리겠습니다. 개정이유는 참전유공자와 그 유족에게 합당한 예우를 하기 위하여 월남참전유공자에게 참전명예수당을 매월 10만원으로 인상하여 지급하고 참전유공자가 사망한 경우 그 배우자에게 매월 5만원을 지급하는 내용입니다. 주요 내용으로는 월남참전유공자 참전명예수당의 경우 2018년 2월 9일 조례ㆍ규칙심의회에서 수정 가결되어 매월 10만원으로 2018년도 1월부터 소급하여 지급하는 것으로 변경되었으며, 참전유공자 유족수당은 원안대로 참전유공자의 배우자에게 매월 5만원 지급입니다. 조례 개정안, 신ㆍ구조문대비표, 관계법령 발췌는 붙임을 참고하시기 바랍니다. 성별영향분석평가 결과 특이사항은 없으며, 규제심사 해당사항 없습니다. 조례ㆍ규칙심의회는 2018년 2월 9일 수정 가결되었습니다. 비용추계서는 붙임을 참고하시고, 1월 26일부터 2월2일까지 입법예고한 결과 의견제출 없었습니다. 제안자 의견입니다. 참전유공자와 그 유족에게 합당한 예우를 하여 군민의 나라사랑정신 함양에 이바지 하고자 본 조례를 개정하는 것으로 원안대로 심의ㆍ의결하여 주시기 바랍니다. 다음은 음성군 국가보훈대상자 예우 및 지원에 관한 조례 일부개정조례안에 대하여 설명드리겠습니다. 개정이유는 국가보훈대상자 유족에 대한 합당한 예우를 하기 위하여 보훈예우수당 중 유족수당의 수급대상자를 배우자와 배우자가 없는 경우 보훈대상자의 부모까지 확대하여 생활안정에 기여하고자 하는 내용입니다. 주요 내용으로는 유족수당의 대상을 유족인 배우자에서 배우자가 없는 경우 보훈대상자의 부모까지 확대하는 것입니다. 조례 개정안, 신ㆍ구조문대비표, 관계법령 발췌는 붙임을 참고하시기 바랍니다. 성별영향분석평가 결과 특이사항이 없으며, 규제심사는 해당사항 없습니다. 조례ㆍ규칙심의회는 2018년 2월 9일 원안 가결되었습니다. 비용추계서는 붙임을 참고하시고, 1월 26일부터 2월 2일까지 입법예고한 결과 의견제출 없었습니다. 제안자 의견입니다. 국가보훈대상자에서 합당한 예우를 하여 군민의 나라사랑 정신함양에 이바지하고자 본 조례를 개정하는 것으로 원안대로 심의ㆍ의결하여 주시기 바랍니다. 다음은 본 안건에 대하여 전문위원으로부터 일괄 검토보고를 듣겠습니다. 전문위원 황의승입니다. 주민지원과 소관으로 의안번호 제396호, 음성군 참전유공자 지원 조례 일부개정조례안과 의안번호 제397호, 음성군 국가보훈대상자 예우 및 지원에 관한 조례 일부개정조례안에 대하여 일괄 검토보고드리겠습니다. 본 개정 조례안은 2018년 2월 12일 의원간담회에서 설명이 있었으며, 「지방자치법」제39조 규정에 의한 지방의회의 의결사항입니다. 검토의견입니다. 참전유공자 지원 조례 일부개정조례안은 월남참전유공자와 그 유족의 합당한 대우를 위하여 참전명예수당을 인상하고 참전유공자가 사망한 경우에는 그 배우자에게 수당을 지급하는 것으로 원안대로 의결함이 타당하다고 판단됩니다. 음성군 국가보훈대상자 예우 및 지원에 관한 일부개정조례안은 국가보훈대상자 유족에게 합당한 예우를 위하여 유족수당의 수급대상자를 배우자가 없는 경우 부모까지 수급가능토록 확대하는 것으로 원안대로 의결함이 타당하다고 판단됩니다. 의사일정 제9항, 음성군 참전유공자 지원 조례 일부개정조례안을 원안대로 의결하고자 하는데, 이의가 없으므로, 음성군 참전유공자 지원 조례 일부개정조례안이 원안대로 가결되었음을 선포합니다.'"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "summary:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'음성군 참전유공자 지원 조례 일부개정조례안은 참전 유공자와 그 유족에게 합당한 예우를 하기 위하여 참전명예수당을 매월 10만원으로 인상하고 참전유의공자가 사망한 경우 그 배우자에게 매월 5만원을 지급하는 내용이며, 해당 안건은 가결됨. 음성군 국가보훈대상자 예우 및 지원에 관한 조례 일부 개정조례안이 가결되었음.'"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "WuJsAT8WAsJi",
        "outputId": "208131cf-31cb-48c9-896d-4eb597955e36"
      },
      "source": [
        "id=66\n",
        "print('Context:\\n')\n",
        "display(test_real['title_context'][id])\n",
        "\n",
        "print('\\nsummary:\\n')\n",
        "display(sub['summary'][id])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'제235회 완주군의회(제2차 정례회) 제1차 본회의: 다음은 의사일정 제3항 완주군수 및 관계공무원 출석요구의 건을 상정합니다. 본 건에 대하여 발의하신 정종윤 의원님은 나오셔서 제안설명 해주시기 바랍니다. 존경하는 최등원 의장님과 선배 동료 의원 여러분! 그리고 박성일 군수님을 비롯한 관계 공무원 여러분! 구이·상관·소양 출신 더불어 민주당 정종윤 의원입니다. 본 안건은 지방자치법 제42조제2항 및 완주군의회 회의규칙 제66조 규정에 따른 것으로 이번 제2차 정례회 기간 중에 실시되는 군정질문에 대해서 집행부 측의 책임 있는 답변을 듣기 위해 다가오는 11월 30일 제2차 본회의에 완주군수 등 관계 공무원들의 출석을 요구하는 것입니다. 군정질문은 집행부가 추진 중인 주요 현안사업 등에 대한 질문으로 그동안 우리 의원님들께서 의정활동을 통해 수집한 자료와 군민들의 의견을 바탕으로 집행부에서 추진하거나 계획 중인 시책들의 문제점을 도출하고 발전적 대안을 제시함은 물론, 군민들의 대의기관인 완주군의회의 의사를 집행부에 전달하기 위한 것으로 집행부의 책임 있는 답변을 기대하며 완주군수 및 관계공무원 출석요구의 건에 대한 제안 설명을 마치겠습니다. 아무쪼록 제안설명 드린 원안대로 의결될 수 있도록 우리 의원님들의 협조를 부탁드립니다. 그러면 정종윤 의원님께서 제안 설명한 안건에 대하여 이의가 없으면 원안대로 의결하고자 하는데 이의가 없으므로 가결되었음을 선포합니다.'"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "summary:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'제235회 완주군의회 제2차 정례회 제1차 본회의에 완주군수 등 관계공무원의 출석을 요구하여 가결됨.'"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}